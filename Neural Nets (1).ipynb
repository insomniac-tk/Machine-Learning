{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used [this](http://neuralnetworksanddeeplearning.com/chap1.html \"Neural networks and deep learning\") as a reference for this notebook.<br>\n",
    "Many thanks to the author Micheal Nielsen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test!\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A perceptron takes several binary inputs, x1,x2,…, and produces a single binary output</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt proposed a simple rule to compute the output. He introduced weights, w1,w2,…, real numbers expressing the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined by whether the weighted sum ∑jwjxj is less than or greater than some threshold value.<br>Just like the weights, the threshold is a real number which is a parameter of the neuron. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron](https://www.saedsayad.com/images/Perceptron_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the basic mathematical model. A way you can think about the perceptron is that it's a device that makes decisions by **weighing up evidence.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jim is fond of eminem and wants to attend his newest concert in Nottingham. He can't wait to go. Although, his decision depends on three factors: <br> \n",
    "  1. The weather , Jim hates bad weather. \n",
    "  2. Whether he has someone to go along with him. Jim doesn't care as much about this though.\n",
    "  3. Whether he'll be able to arrange the necessarry means of transport for that day. \n",
    "Here, x1 = weather , x2= IsHeALone , x3 = DoesHeHaveTheMeans\n",
    "\n",
    "His decision to go or not (1/0) depends upon these three factors and how **important** each is with respect to the other.<br>\n",
    "Say w1 = 5(Very important to have good weather) , w2 = 1 (He doesn't care if he goes solo) , w3 = 4 (transport,imp!)\n",
    "Say threshold is 6.\n",
    "Consider a day with good weather and no friends , and a lack of public transport \n",
    "\n",
    "So, will be go or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "No shady for him today.\n",
      "-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perceptron(weights,threshold,inputs):\n",
    "    sum = 0\n",
    "    for i in range(len(inputs)):\n",
    "        sum += weights[i]*inputs[i]\n",
    "    print(sum)\n",
    "    return 1 if sum > threshold else 0\n",
    "\n",
    "weights = [5,1,4]\n",
    "inputs = [1,0,0]\n",
    "\n",
    "\n",
    "output = perceptron(weights,6,inputs)\n",
    "if output == 1:\n",
    "    print(\"He is going to see Shady!\")\n",
    "else:\n",
    "    print(\"No shady for him today.\")\n",
    "perceptron([-2,-2],-3,[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too bad for him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons and Logic Gates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nand Gate\n",
    "Truth Table for NAND :  \n",
    " 0 0  -> 1<br>\n",
    " 0 1  -> 1<br>\n",
    " 1 0  -> 1<br>\n",
    " 1 1  -> 0  \n",
    " <br>\n",
    " consider a perceptron with two weights , each -2 and a threshold of -3<br>\n",
    " So, we have :  \n",
    " x1.w1 + x2.w2 > -3 , for firing  [1]  \n",
    " x1.w1 + x2.w2 <= -3 , for not firing  [2]  \n",
    " \n",
    " Enter bias.  \n",
    " Let's put 0 on the other side!  \n",
    " bias = -threshold  \n",
    " So, 1 and 2 become :  \n",
    " \n",
    " x1.w1 + x2.w2 + 3 > 0  \n",
    " x1.w1 + x2.w2 + 3 <=0\n",
    " <hr>\n",
    " ![NAND GATE](http://neuralnetworksanddeeplearning.com/images/tikz2.png)\n",
    " <hr>\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] -> 1\n",
      "[0, 1] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "def perceptron(weights,bias,inputs):  #bias takes over threshold\n",
    "    sum = weights[0]*inputs[0] + weights[1]*inputs[1] + bias\n",
    "    return 1 if sum > 0 else 0  # w.x + b\n",
    "\n",
    "weights = [-2,-2]\n",
    "inputs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "bias = 3\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    output = perceptron(weights,bias,inputs[i])\n",
    "    print(inputs[i],\"->\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neurons\n",
    "![Sigmoid Neuron](http://neuralnetworksanddeeplearning.com/images/tikz9.png)\n",
    "<hr>\n",
    "Just like perceptrons, this boi also has inputs , but these are not binary.<br>\n",
    "They take any value between 0 to 1. Here output isn't just 0 or 1, it is define by the sigmoid function.<br>\n",
    "\n",
    "By using the actual σ function we get, as already implied above, a smoothed out perceptron. Indeed, it's the smoothness of the σ function that is the crucial fact, not its detailed form. The smoothness of σ means that small changes Δwj in the weights and Δb in the bias will produce a small change Δoutput in the output from the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small change in the weight or bias will produce a small change in the output. Calculus tells us that, change in output can be approximated as : <br> \n",
    "\\begin{eqnarray} \n",
    "  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\tag{5}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **change in output** is a linear function of change in output wrt weight and bias respectively. <br>\n",
    "So while sigmoid neurons have much of the same qualitative behaviour as perceptrons, they make it much easier to figure out how changing the weights and biases will change the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Excercises!\n",
    "\n",
    "1. Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c>0. Show that the behaviour of the network doesn't change. \n",
    "\n",
    "  Solution :   \n",
    "  Consider W as the weight vector, X as input vector and a bias B. To fire , sigmoid(W.X + B) must be greater than  \n",
    "  zero. \n",
    "  After multiplying bias and input by c, we've :  \n",
    "  W' = c.W , B' = c.B\n",
    "  Output is the sigmoid of weighted sum of inputs and the bias.\n",
    "  Sum(W'(i).X(i) + b) \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Neural Net\n",
    "\n",
    "3 layers <br>\n",
    "\n",
    "1. input layer - 784 neurons ( 28x28)<br> \n",
    "   Input neurons abstract pixels on a grayscale 28x28 image.<br>\n",
    "2. Hidden layer \n",
    "   15 Neurons\n",
    "3. Output Layer\n",
    "   10 Neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what now?\n",
    "Now we have three steps :\n",
    "1. Load dataset.\n",
    "2. Train.\n",
    "   2.1 Prepare Model \n",
    "   2.2 Decide Learning Algorithm\n",
    "3. Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation \n",
    "1. Input\n",
    "   each training input 'x' will be denoted as a 784 dimensional vector\n",
    "2. Output\n",
    "   a 10-dimensional vector \n",
    "    <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>y</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>1</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>0</mn>\n",
    "  <msup>\n",
    "    <mo stretchy=\"false\">)</mo>\n",
    "    <mi>T</mi>\n",
    "  </msup>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
